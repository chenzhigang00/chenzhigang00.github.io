1:"$Sreact.fragment"
2:I[75068,["/audio/_next/static/chunks/38a6e0ded54c1e98.js","/audio/_next/static/chunks/dc2bd5161e8ef372.js"],"default"]
3:I[26207,["/audio/_next/static/chunks/38a6e0ded54c1e98.js","/audio/_next/static/chunks/dc2bd5161e8ef372.js"],"default"]
4:I[92188,["/audio/_next/static/chunks/ecec75432ba00b1a.js","/audio/_next/static/chunks/99963addf049a8f8.js"],"Navbar"]
5:I[77268,["/audio/_next/static/chunks/ecec75432ba00b1a.js","/audio/_next/static/chunks/99963addf049a8f8.js"],"NoteCard"]
10:I[94148,[],"default"]
:HL["/audio/_next/static/chunks/e960086a9e8ef03c.css","style"]
:HL["/audio/_next/static/media/2f13ad8e538108e0-s.p.9b936cf7.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/audio/_next/static/media/83afe278b6a6bb3c-s.p.3a6ba036.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
0:{"P":null,"b":"TNWS3laKXu_pzGCJPsLOV","c":["","notes",""],"q":"","i":false,"f":[[["",{"children":["notes",{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/audio/_next/static/chunks/e960086a9e8ef03c.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"dark","children":["$","body",null,{"className":"inter_fe8b9d92-module__LINzvG__variable roboto_mono_a48da9b6-module__RNpGXW__variable antialiased bg-background text-foreground min-h-screen selection:bg-primary/20 selection:text-primary","children":["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[["$","div",null,{"className":"min-h-screen flex flex-col","children":[["$","$L4",null,{}],["$","main",null,{"className":"flex-1 pt-24 pb-12","children":["$","div",null,{"className":"container px-4 md:px-6 mx-auto","children":[["$","div",null,{"className":"flex flex-col space-y-4 mb-12","children":[["$","h1",null,{"className":"text-3xl font-bold tracking-tighter sm:text-5xl bg-clip-text text-transparent bg-gradient-to-r from-blue-400 to-purple-500","children":"Research Notes"}],["$","p",null,{"className":"max-w-[700px] text-muted-foreground md:text-xl","children":"A complete archive of our research findings, technical breakdowns, and tutorials."}]]}],["$","div",null,{"className":"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6","children":[["$","$L5","asvspoof-2021-review",{"note":{"slug":"asvspoof-2021-review","content":"\n# ASVspoof 2021 Dataset Review\n\nThe ASVspoof challenge is the premier benchmark for audio deepfake detection. The 2021 edition introduced new challenges, including deepfake speech in compressed codecs and physical access attacks.\n\n## Logical Access (LA)\n\nThis scenario focuses on synthetic speech (TTS) and voice conversion (VC) attacks transmitted over a telephone or VoIP channel.\n\n## Physical Access (PA)\n\nThis scenario involves replay attacks, where a recording of the target speaker is played back through a device in the same physical space as the microphone.\n\n## Speech Deepfake (DF)\n\nA new track focused on detecting compressed deepfakes, simulating the scenario where fake audio is shared over social media platforms that apply lossy compression.\n","id":"asvspoof-2021-review","title":"Audio Deepfake Dataset Review: ASVspoof 2021","excerpt":"A comprehensive review of the ASVspoof 2021 challenge dataset, focusing on logical access and physical access scenarios.","date":"2024-02-28","readTime":"9 min","tags":["Dataset","ASVspoof","Evaluation"]},"index":0}],"$L6","$L7","$L8","$L9","$La"]}]]}]}],"$Lb"]}],["$Lc","$Ld"],"$Le"]}],{},null,false,false]},null,false,false]},null,false,false],"$Lf",false]],"m":"$undefined","G":["$10",[]],"S":true}
12:I[16432,["/audio/_next/static/chunks/38a6e0ded54c1e98.js","/audio/_next/static/chunks/dc2bd5161e8ef372.js"],"OutletBoundary"]
13:"$Sreact.suspense"
15:I[16432,["/audio/_next/static/chunks/38a6e0ded54c1e98.js","/audio/_next/static/chunks/dc2bd5161e8ef372.js"],"ViewportBoundary"]
17:I[16432,["/audio/_next/static/chunks/38a6e0ded54c1e98.js","/audio/_next/static/chunks/dc2bd5161e8ef372.js"],"MetadataBoundary"]
6:["$","$L5","watermarking-audio-defense",{"note":{"slug":"watermarking-audio-defense","content":"\n# Watermarking Audio\n\nWatermarking involves embedding a hidden signal into the audio that survives compression and transmission but can be decoded to verify authenticity or origin.\n\n## Techniques\n\n*   **Spread Spectrum**: Hiding the watermark signal across a wide frequency band.\n*   **Echo Hiding**: Introducing imperceptible echoes to encode data.\n*   **Deep Watermarking**: Using neural networks to encode and decode watermarks robustly.\n\n## Application in Generative AI\n\nIf all generative models were required to embed a watermark, we could easily identify synthetic content. Initiatives like C2PA are working towards standards for content provenance.\n","id":"watermarking-audio-defense","title":"Watermarking Audio: A Defense Strategy","excerpt":"Implementing imperceptible watermarks in audio to prove authenticity and trace the origin of synthetic content.","date":"2024-01-12","readTime":"4 min","tags":["Watermarking","Provenance","Defense"]},"index":1}]
7:["$","$L5","adversarial-attacks-detectors",{"note":{"slug":"adversarial-attacks-detectors","content":"\n# Adversarial Attacks on Deepfake Detectors\n\nJust as AI can be used to generate deepfakes, it can also be used to evade detection. Adversarial attacks involve adding imperceptible noise to an audio file to flip the detector's decision.\n\n## Types of Attacks\n\n1.  **White-box attacks**: The attacker has full access to the detector's model weights.\n2.  **Black-box attacks**: The attacker can only query the detector and observe the output.\n\n## Vulnerability\n\nOur experiments show that even state-of-the-art detectors like RawNet2 can be fooled by adding noise with an SNR of 40dB, which is virtually inaudible to the human ear.\n\n## Robust Training\n\nTo defend against these attacks, we propose training detectors with adversarial examples included in the training set (adversarial training).\n","id":"adversarial-attacks-detectors","title":"Adversarial Attacks on Deepfake Detectors","excerpt":"Investigating how robust current deepfake detection models are against adversarial perturbations. Can we fool the detectors?","date":"2023-12-05","readTime":"7 min","tags":["Adversarial ML","Robustness","Security"]},"index":2}]
8:["$","$L5","zero-shot-voice-conversion",{"note":{"slug":"zero-shot-voice-conversion","content":"\n# The Rise of Zero-Shot Voice Conversion\n\nZero-shot voice conversion (VC) allows for cloning a speaker's voice without any fine-tuning, using only a few seconds of reference audio.\n\n## How it Works\n\nThese models typically disentangle the linguistic content from the speaker identity. During inference, the content from a source utterance is combined with the speaker embedding from a target reference.\n\n## Implications\n\nThe ease of use and speed of these models pose significant security risks:\n\n*   **Biometric Bypass**: Voice authentication systems can be easily fooled.\n*   **Social Engineering**: Attackers can impersonate trusted individuals (CEO fraud, family emergencies).\n\n## Countermeasures\n\nWe need to develop more robust liveness detection and anti-spoofing measures that go beyond simple speaker verification.\n","id":"zero-shot-voice-conversion","title":"The Rise of Zero-Shot Voice Conversion","excerpt":"Analyzing the latest advancements in zero-shot voice conversion models that require only a few seconds of reference audio. Implications for security and privacy.","date":"2023-11-20","readTime":"6 min","tags":["Voice Conversion","Security","Zero-Shot"]},"index":3}]
11:T439,
# Detecting AI Voice Clones

Modern voice cloning tools have become incredibly realistic, but they are not perfect. They often leave behind subtle spectral artifacts that can be detected with the right tools.

## Spectral Artifacts

When analyzing the spectrograms of synthetic audio, we often observe:

*   **High-frequency cutoff**: Many models struggle to generate realistic high-frequency content.
*   **Checkerboard artifacts**: Resulting from deconvolution operations in neural vocoders.
*   **Phase inconsistencies**: The phase information in generated audio is often less coherent than in natural speech.

## Tools for Analysis

*   **Mel-spectrograms**: Visualizing the frequency content over time.
*   **Constant-Q Transform (CQT)**: Useful for analyzing musical or tonal content.
*   **Bispectral Analysis**: detecting non-linearities introduced by the generation process.

## Case Study: ElevenLabs

We analyzed 100 samples generated by ElevenLabs and found distinct patterns in the 8kHz-10kHz range that are absent in real human speech recordings of similar quality.
9:["$","$L5","detecting-voice-clones-spectral",{"note":{"slug":"detecting-voice-clones-spectral","content":"$11","id":"detecting-voice-clones-spectral","title":"Detecting AI Voice Clones: Spectral Analysis Techniques","excerpt":"Exploring the subtle spectral artifacts left behind by modern voice cloning tools like ElevenLabs and Tortoise TTS. Can we spot the synthetic signature?","date":"2023-11-02","readTime":"8 min","tags":["Detection","Forensics","Spectral Analysis"]},"index":4}]
a:["$","$L5","understanding-wavegrad",{"note":{"slug":"understanding-wavegrad","content":"\n# Understanding WaveGrad\n\nWaveGrad is a conditional model for waveform generation which estimates gradients of the data density. It is built on the concept of diffusion probabilistic models and score matching.\n\n## Key Concepts\n\n1.  **Diffusion Models**: WaveGrad defines a forward process that adds noise to the data and a reverse process that denoises it.\n2.  **Gradient Estimation**: The core idea is to estimate the gradient of the log-density of the data distribution.\n\n## Comparison with WaveNet\n\nUnlike WaveNet, which is autoregressive, WaveGrad is non-autoregressive, allowing for faster inference in some settings, although it requires multiple iterations (steps) to generate high-quality audio.\n\n## Conclusion\n\nWaveGrad represents a significant step forward in high-fidelity audio synthesis, offering a trade-off between inference speed and sample quality through the number of refinement steps.\n","id":"understanding-wavegrad","title":"Understanding WaveGrad: Audio Synthesis Fundamentals","excerpt":"A deep dive into WaveGrad, a conditional model for waveform generation through estimating gradients of the data density. How does it compare to WaveNet?","date":"2023-10-15","readTime":"5 min","tags":["Synthesis","Generative Models","WaveGrad"]},"index":5}]
b:["$","footer",null,{"className":"border-t border-border bg-background/95 backdrop-blur supports-[backdrop-filter]:bg-background/60","children":["$","div",null,{"className":"container flex flex-col items-center justify-between gap-4 py-10 md:h-24 md:flex-row md:py-0","children":["$","div",null,{"className":"flex flex-col items-center gap-4 px-8 md:flex-row md:gap-2 md:px-0","children":["$","p",null,{"className":"text-center text-sm leading-loose text-muted-foreground md:text-left","children":["Built by"," ",["$","a",null,{"href":"#","target":"_blank","rel":"noreferrer","className":"font-medium underline underline-offset-4","children":"DeepGuard"}],". The source code is available on"," ",["$","a",null,{"href":"#","target":"_blank","rel":"noreferrer","className":"font-medium underline underline-offset-4","children":"GitHub"}],"."]}]}]}]}]
c:["$","script","script-0",{"src":"/audio/_next/static/chunks/ecec75432ba00b1a.js","async":true,"nonce":"$undefined"}]
d:["$","script","script-1",{"src":"/audio/_next/static/chunks/99963addf049a8f8.js","async":true,"nonce":"$undefined"}]
e:["$","$L12",null,{"children":["$","$13",null,{"name":"Next.MetadataOutlet","children":"$@14"}]}]
f:["$","$1","h",{"children":[null,["$","$L15",null,{"children":"$L16"}],["$","div",null,{"hidden":true,"children":["$","$L17",null,{"children":["$","$13",null,{"name":"Next.Metadata","children":"$L18"}]}]}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]
16:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
19:I[99864,["/audio/_next/static/chunks/38a6e0ded54c1e98.js","/audio/_next/static/chunks/dc2bd5161e8ef372.js"],"IconMark"]
14:null
18:[["$","title","0",{"children":"DeepGuard | AI Audio Deepfake Detection"}],["$","meta","1",{"name":"description","content":"Research notes and resources on AI audio deepfake detection, synthesis, and forensics."}],["$","link","2",{"rel":"icon","href":"/audio/favicon.ico?favicon.0b3bf435.ico","sizes":"256x256","type":"image/x-icon"}],["$","$L19","3",{}]]
